{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recogniser using Convolutional Neural Network\n",
    "\n",
    "\n",
    "In this workshop we are going a develop and train a deep neural network called convolutional neural network to recognise handwritten digits. We'll be using the MNIST (\"Modified National Institute of Standards and Technology\") dataset, also considered as the \"Hello World\" dataset in computer vision/deep learning. It contains a large set of human handwritten and annotated digit images.\n",
    "\n",
    "By the end of the tutorial you will have learnt how convolutional neural networks work, how to deploy pre-trained model, how to visualise the results. We'll build this neural net using tensorflow and deploy the pretrained model as rest api."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and Verify TensorFlow is Installed\n",
    "\n",
    "We'll start of by first importing the libraries that are required for this project. The Deep learning framework we are using is tensorflow. This is even to check if the correct version of tensorflow is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Data Set\n",
    "\n",
    "MNIST Data Set is a collection of handwritten digits images comprising 60,000 training images and 10,000 test images. It usually used as a benchmark for the new computer vision and pattern recognition algorithms. The images here have been size-normalised and centered in fixed size image.\n",
    "\n",
    "Tensorflow has a module which helps us download this particular dataset, split it into training set, validation set, test set and creates one-hot vectors for labels of each image. The training set contains 55,000 images, validation set has 5,000 and test set has 10,000 images.\n",
    "\n",
    "**So why is it important to separate data into 3 sets?**\n",
    "\n",
    "These images here are 2D images each consisting an array of 28x28 values that have been flattened to get a rich structure of 784 dimensional vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"No. of Training Examples : \",mnist.train.num_examples)\n",
    "print(\"No. of Validation Examples : \",mnist.validation.num_examples)\n",
    "print(\"No. of Test Examples : \",mnist.test.num_examples)\n",
    "print(\"Example of a one-hot encoded vector : \\n\",mnist.train.labels[0])\n",
    "print(\"Flattened Image Shape : \", mnist.train.images[0].shape)\n",
    "print(\"Example of a flattened image: \\n\", mnist.train.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_array = mnist.train.images[np.random.randint(mnist.train.images.shape[0])]\n",
    "img_array = 255 * img_array\n",
    "img_array = img_array.astype(\"uint8\")\n",
    "#print(img_array.reshape([28,28]))\n",
    "plt.imshow(img_array.reshape([28,28]))\n",
    "plt.gray()\n",
    "plt.grid(True)\n",
    "plt.savefig('test.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build our network\n",
    "\n",
    "You'll have to build the necessary components given below alongwith us for designing the convolutional neural network.\n",
    "- `input_placeholders`\n",
    "- `init_weights_bias`\n",
    "- `conv2d`\n",
    "- `max_pool`\n",
    "- `flatten`\n",
    "- `fcn`\n",
    "- `output`\n",
    "- `cnn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_placeholders():\n",
    "    \"\"\"\n",
    "    Create TF placeholders for inputs, targets, keep_prob, learning_rate\n",
    "    Return: A Tuple(inputs, targets, keep_prob, learning_rate)\n",
    "    \"\"\"\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Weights and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights_bias(w_shape, b_shape):\n",
    "    \"\"\"\n",
    "    w_shape: shape for weight variable\n",
    "    b_shape: shape for the bias variable\n",
    "    Create tensorflow variables for weights and biases.\n",
    "    Initialize using tf.truncated_normal()\n",
    "    Return: (weights, bias)\n",
    "    \"\"\"\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(inputs, weights, stride):\n",
    "    \"\"\"\n",
    "    Apply convolution operation to inputs tensor.\n",
    "    inputs: tensorflow tensor\n",
    "    weights: weights initialised for convolution operation\n",
    "    stride: 2d tuple for convolution\n",
    "    \"\"\"\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool(inputs, kernel_size, stride):\n",
    "    \"\"\"\n",
    "    Apply max pooling operation to inputs tensor.\n",
    "    inputs: tensorflow tensor\n",
    "    kernel_size: 2d tuple for pooling\n",
    "    stride: 2d tuple for pooling\n",
    "    \"\"\"\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(inputs):\n",
    "    \"\"\"\n",
    "    Flatten inputs tensor to (batch_size, flattened_image_size).\n",
    "    \"\"\"\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fcn(inputs, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to inputs tensor using weights and bias\n",
    "    inputs: tensorflow tensor\n",
    "    num_outputs: number of outputs the new tensor should be.\n",
    "    \"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outputs(inputs, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply an output layer to inputs tensor using weights and bias\n",
    "    inputs: tensorflow tensor\n",
    "    num_outputs: number of outputs the new tensor should be.\n",
    "    \"\"\"\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Convolutional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn(inputs, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional model using inputs tensor and use keep_prob while applying dropout \n",
    "    \"\"\"\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_stats(sess, batch_train, batch_train_labels, batch_val, batch_val_labels, cost, accuracy, writer, step):\n",
    "    \"\"\"\n",
    "    Display intermediate results.\n",
    "    \"\"\"\n",
    "    train_data = sess.run([accuracy, summ, cost], feed_dict={\n",
    "        inputs: batch_train, targets: batch_train_labels, keep_prob: 1.0, learning_rate:0.0001})\n",
    "    val_data =sess.run([accuracy, cost], feed_dict={\n",
    "        inputs: batch_val, targets: batch_val_labels, keep_prob: 1.0, learning_rate:0.0001})\n",
    "    \n",
    "    writer.add_summary(train_data[1])\n",
    "    \n",
    "    print(\"Step {} Train_Loss : {:>10.4f} Train_Accuracy : {:.6f} Val_Loss : {:>10.4f} Val_Accuracy : {:.6f}\"\n",
    "         .format(step, train_data[2], train_data[0], val_data[0], val_data[1]))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#Initialise Session\n",
    "sess = tf.Session()\n",
    "\n",
    "#Inputs\n",
    "inputs, targets, keep_prob, learning_rate = input_placeholders()\n",
    "\n",
    "#Model\n",
    "logits = cnn(inputs, keep_prob)\n",
    "\n",
    "#Loss Function\n",
    "with tf.name_scope(\"Loss_Function\"):\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=targets, logits=logits))\n",
    "    tf.summary.scalar(\"loss\", cross_entropy)\n",
    "\n",
    "#Optimizer\n",
    "with tf.name_scope(\"Optimizer\"):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# Report Accuracy\n",
    "with tf.name_scope(\"Accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(targets, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "\n",
    "#Define variable to save summaries\n",
    "summ = tf.summary.merge_all()\n",
    "\n",
    "#Initialise all variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#Create a saver object to save the model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#Write Graph to Tensorboard\n",
    "!rm -r log/\n",
    "writer = tf.summary.FileWriter(\"log/\")\n",
    "writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "for i in range(2000):\n",
    "    batch = mnist.train.next_batch(16)\n",
    "    # Display stats for every 100th step\n",
    "    if i % 100 == 0:\n",
    "        display_stats(sess, batch[0], batch[1], mnist.validation.images, mnist.validation.labels, \n",
    "                      cross_entropy, accuracy, writer, i)\n",
    "        saver.save(sess, 'trained/test_model')\n",
    "    sess.run(train_step,feed_dict={inputs: batch[0], targets: batch[1], keep_prob: 0.4, learning_rate:0.0005})\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total training time {:2f}\".format(end-start))\n",
    "\n",
    "print('\\nTest accuracy : {}'.format(sess.run(accuracy,feed_dict={\n",
    "    inputs: mnist.test.images[:100], targets: mnist.test.labels[:100], keep_prob: 1.0, learning_rate:0.0001})))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using the Trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "saver=tf.train.import_meta_graph('trained/test_model.meta')\n",
    "saver.restore(sess, tf.train.latest_checkpoint('./trained/'))\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "output = graph.get_tensor_by_name(\"output_fc_layer/outputs:0\")\n",
    "\n",
    "inputs = graph.get_tensor_by_name(\"Inputs:0\")\n",
    "targets = graph.get_tensor_by_name(\"Labels:0\")\n",
    "keep_prob = graph.get_tensor_by_name(\"keep_prob:0\")\n",
    "learning_rate = graph.get_tensor_by_name(\"learning_rate:0\")\n",
    "\n",
    "pred = tf.nn.softmax(output)\n",
    "img_predict_index = np.random.randint(mnist.test.images.shape[0])\n",
    "img_array = 255 * mnist.test.images[img_predict_index]\n",
    "img_array = img_array.astype(\"uint8\")\n",
    "plt.imshow(img_array.reshape([28,28]))\n",
    "plt.gray()\n",
    "\n",
    "predictions = sess.run(pred, feed_dict={inputs:mnist.test.images[img_predict_index].reshape(1,784), \n",
    "                                        targets: mnist.test.labels[img_predict_index].reshape(1,10), \n",
    "                                        keep_prob:1.0, learning_rate:0.0001})\n",
    "\n",
    "print(np.argmax(predictions[0]))\n",
    "\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
